The ipynb file here achieves an accuracy of 99.4 for the MNIST handwritten dataset using Pytorch Deeplearning Framework.

The network includes Convolution layers, Max pooling & Global average pooling with Batch normalization & Dropouts to achieve the above mentioned accuracy.

Here below are the Log of 20 epochs. The highest validation accuracy it clocked is 99.46

Logs for 20 epochs

loss=0.05680118873715401 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.01it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0845, Accuracy: 9734/10000 (97.34%)

loss=0.022368302568793297 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.87it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0487, Accuracy: 9852/10000 (98.52%)

loss=0.029159652069211006 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.63it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0412, Accuracy: 9859/10000 (98.59%)

loss=0.08382397890090942 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.69it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0361, Accuracy: 9887/10000 (98.87%)

loss=0.04316417872905731 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.59it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0297, Accuracy: 9912/10000 (99.12%)

loss=0.007528558373451233 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 35.27it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0294, Accuracy: 9911/10000 (99.11%)

loss=0.05227547883987427 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.49it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0271, Accuracy: 9925/10000 (99.25%)

loss=0.16345296800136566 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.35it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0458, Accuracy: 9860/10000 (98.60%)

loss=0.01780090294778347 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.60it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0275, Accuracy: 9911/10000 (99.11%)

loss=0.05488380417227745 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.90it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0198, Accuracy: 9934/10000 (99.34%)

loss=0.03938816860318184 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.04it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0230, Accuracy: 9931/10000 (99.31%)

loss=0.03761656954884529 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 33.88it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0255, Accuracy: 9922/10000 (99.22%)

loss=0.005949988961219788 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.53it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0269, Accuracy: 9918/10000 (99.18%)

loss=0.009693201631307602 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.28it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0219, Accuracy: 9930/10000 (99.30%)

loss=0.004258033353835344 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 33.97it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0178, Accuracy: 9946/10000 (99.46%)

loss=0.007672844920307398 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.37it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0202, Accuracy: 9941/10000 (99.41%)

loss=0.05448484420776367 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 33.71it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0266, Accuracy: 9925/10000 (99.25%)

loss=0.018805811181664467 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.18it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0223, Accuracy: 9934/10000 (99.34%)

loss=0.0038175806403160095 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.40it/s]

Test set: Average loss: 0.0179, Accuracy: 9941/10000 (99.41%)
